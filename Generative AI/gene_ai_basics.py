# In this section we will start with an inttroduction to Generative AI, and LLM's.

# We will be discussing things like Open AI and how to generate Open AI keys, how to use the Open AI API, Understanding
#Chat Completion API, Functional API, Understanding the concept of tokens, What is a token, How many tokens should we 
#use whenever we are giving a prompt, what are the different prompt templates, and many more things.


# We will begin by talking about Generativve AI and the history of LLM's.

# Whenever we are talking about Generative AI, or a LLM,a couple of names are very famous now a days, Chat GPT, Google
#Bard, and Meta Llama 2.
 
# Chat GPT - A product of the Open AI.

# Google Bard - A prduct of Google.

# Meta Llama 2 - A product of Facebook.

# Even though these names areassociated with Gen AI, Gen AI has its own rules.

# It's not all about chay gpt, google bard, meta llama 2 or other applications which we might see, these are just 
#applications of Gen AI, or LLM.

# Basically we are using LLM in the backend of whatever application we are seeing, like chat gpt, for example.

# Now let's look at a few concepts of deep learning, because we need to understand some terms and terminology 
#regarding deep learning.

# These will be our basics of deep learning.

# Things like the types of neural networks, and also LLM's.

# So if we're talking about deep learning we can divide this deep learning into three major segments.

# The first topic is Artificial Neural Networks - ANN

# The second topic is callen Convolution Neural Networks - CNN

# The third topic is called Recurrent Neural Network - RNN

# These are our the three types of neural network we can divide deep learning into.

# Apart from these, we will find out that there are other types we may encounter as well.

# These are the topics like...

# Fourth is Reinforcement Learning - RL

# Fifth is Generative Adverserial Network - GAN

# First, let's look at the A N N.

# Let's image we have an input layer   [   ]  inside of our A N N.
 
# This input layer [   ] produces an output.

# Inbetween the input and output layers will be a hidden layer.

#             _____
# ---------> |_____| --------->  
# input    Hidden Layer    Output

# The hidden doesn't actually do anything, it's considered a hyper parameter.

# We can have as many hidden layers as we want.

# Note: When we are talking about A N N, we are using things like structured data, numerical features, or categorical
#features, and we are trying to solve the regression in classification problems.

# But when we are talking about C N N, we will specifically be using this for things like images, and video data.

# We could say that we use the C N N for the grid type of data.

# We will also be using a conponent called Convolution.

# There are various steps we will use in the convolution.

# The very first step in the convolution we will perform is the feature extraction.

# After that, the next step we perform is the pooling.

# Then we flatten the layer.

# After that we will apply that fully connected layer, which is our A N N.

# Convolution + A N N.

# So our first architecture will be this...

# A N N
#             _____
# ---------> |_____| --------->  
# input    Hidden Layer    Output

# And our second architecture will be this...

#C N N
# Convolution + A N N.

# Now when we're talking about the third one, which is very very interesting, that is called the recurrent neural
#network R N N.

# We use the R N N for sequential relative data.

# So whenever we have a sequence, we'll be using our Recurrent Neural Network.

# First, let's draw the architecture of the R N N.

# We have an input, Hidden layer and output as usual, but with the R N N, inside the hidden layer we have something called
#the feed back loop.

#         Feed Back Loop
#             _____
# ---------> |_____| --------->  
# input    Hidden Layer    Output

# What this means is that whatever output we get from output layer we feed back into our hidden state as input.

# We will repeat this process for the entire  time stamp.

# The reason we are going over R N N's is because to fully understand LLM's, we have to understand concepts like 
#"Attention", "Self-Attention", "Encoder-Decoder".

# So here we will briefly going over topics such as, What is a LSTM, What is a GRU, What is Sequence to SEquence mapping,
#Where Attention comes into play, How to implement the Self Attention, How to get started using the Transfer Learning,
#and basically fine tuning our understanding of LLM's.

# Now back to the topic of reinforcement learning.

# In reinforcement learning we go over topics like Agents, Enviornments, States, and feed back.

# This is all apart of deep learning.

# Next we will look at GAN's.

# GAN's are a type of neural network which we use for generating data, and is under the umbrella of Generative AI.

# And we also have diferent types of GAN's.

# These are all types of Neural Networks.